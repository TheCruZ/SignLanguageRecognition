{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4289,
     "status": "ok",
     "timestamp": 1700665956946,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "A87rQqMO-g2k",
    "outputId": "04a5a283-f184-48f2-9a11-ca1563892d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Load imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "from tensorflow.keras                       import regularizers\n",
    "from tensorflow.keras.models                import Sequential, Model\n",
    "from tensorflow.keras.layers                import *\n",
    "from tensorflow.keras.callbacks             import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers            import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61011,
     "status": "ok",
     "timestamp": 1700665838198,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "6FKIDyx5NARF",
    "outputId": "b3b92f47-b142-47af-c186-bf0bf82c48a3"
   },
   "outputs": [],
   "source": [
    "# Base variables for the project\n",
    "DATASET_BASE_PATH = '../Datasets'\n",
    "DATASET_PATH = DATASET_BASE_PATH + '/Letters'\n",
    "TRAINED_MODELS_PATH = '../TrainedModels'\n",
    "DATASET_DRIVE_ID = '1IgH1KCxtoPZNbzNQi4tcYjXY5W2wYAmW' # change this if dataset have been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional if you want to use neptune\n",
    "#execute hidden_neptune.py to get api key\n",
    "exec(open(\"../hidden_neptune.py\").read())\n",
    "using_neptune = True\n",
    "import os\n",
    "import neptune\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune_api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from drive\n",
    "\n",
    "# if dataset folder don't exists download and unzip the dataset\n",
    "import os.path\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    command = 'gdown https://drive.google.com/uc?id=' + DATASET_DRIVE_ID + ' -O dt.zip'\n",
    "    !{command}\n",
    "    command = 'unzip dt.zip -d ' + DATASET_BASE_PATH\n",
    "    !{command}\n",
    "    !rm dt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install tensorrt\n",
    "# tensorrt is a bit special to install and we didn't add it to pipenv\n",
    "\n",
    "#check if tensorrt is installed and install it if not\n",
    "try:\n",
    "    import tensorrt\n",
    "except ImportError:\n",
    "    !pip install tensorrt==8.5.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "bs = 64 #batch size\n",
    "image_side = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7218,
     "status": "ok",
     "timestamp": 1700666025261,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "Kx0Q7OX3SXq2",
    "outputId": "f8900ade-229e-48d5-e607-c239f144f87e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 17:07:56.545757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.546215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.546575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.652215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.652474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.652691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-08 17:07:56.652871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21854 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88008 files belonging to 24 classes.\n",
      "Found 720 files belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset, it must contains Train and Test in categorical folders\n",
    "with tf.device('/cpu:0'):\n",
    "  raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH + \"/Train\",\n",
    "    label_mode = \"categorical\",\n",
    "    shuffle = True,\n",
    "    image_size = (image_side, image_side),\n",
    "    batch_size = bs)\n",
    "\n",
    "  raw_validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH + \"/Test\",\n",
    "    label_mode = \"categorical\",\n",
    "    shuffle = True,\n",
    "    image_size = (image_side, image_side),\n",
    "    batch_size = bs)\n",
    "\n",
    "## split train in 80 and 20 percent for validation\n",
    "## this give fake accuracy\n",
    "# raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#   DATASET_PATH+\"/Train\",\n",
    "#   label_mode = \"categorical\",\n",
    "#   seed=123,\n",
    "#   image_size=(image_side, image_side),\n",
    "#   batch_size=bs,\n",
    "#   validation_split=0.2,\n",
    "#   subset='training',\n",
    "#   )\n",
    "\n",
    "# raw_validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#   DATASET_PATH+\"/Train\",\n",
    "#   label_mode = \"categorical\",\n",
    "#   seed=123,\n",
    "#   image_size=(image_side, image_side),\n",
    "#   batch_size=bs,\n",
    "#   validation_split=0.2,\n",
    "#   subset='validation',\n",
    "#   )\n",
    "\n",
    "# tar without compression to hash the file\n",
    "#command = 'tar -cf ' + DATASET_PATH +'-current.tar ' + DATASET_PATH\n",
    "#!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1700666031305,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "n__WUQGIb56k",
    "outputId": "2848230b-48b0-4ad6-e504-776ad050939e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# define class names\n",
    "class_names = raw_train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1700666037052,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "knUP-tXoSxhO"
   },
   "outputs": [],
   "source": [
    "#normalize from 0-255 to 0-1\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = raw_train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_ds = raw_validation_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 1791,
     "status": "ok",
     "timestamp": 1700666041580,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "2MvgQw6kYZ3X",
    "outputId": "9db53ba3-5545-4fd6-c929-244bf9dad79f"
   },
   "outputs": [],
   "source": [
    "# Show images without data augmentation\n",
    "plt.figure(figsize=(7, 7))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy()) ## use .astype(\"uint8\") if images goes from 0 to 255 else float from 0 to 1\n",
    "    plt.title(class_names[np.argmax(labels[i])])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700667538636,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "5zx8nLTiUnnd"
   },
   "outputs": [],
   "source": [
    "# Config of data augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.01),\n",
    "        tf.keras.layers.RandomZoom(0.02),\n",
    "        tf.keras.layers.RandomTranslation(0.08, 0.08, fill_mode='nearest', fill_value=0.5),\n",
    "        tf.keras.layers.RandomBrightness([-0.15,0.1],value_range=(0, 1)),\n",
    "        #tf.keras.layers.RandomCrop(25,25),\n",
    "        tf.keras.layers.RandomContrast(0.02)\n",
    "    ]\n",
    ")\n",
    "data_augmentation.build((None, image_side, image_side, 3)) ## this is important to prevent usage of data augmentation change his shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 1485,
     "status": "ok",
     "timestamp": 1700667542312,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "G-f3YN6QfSVn",
    "outputId": "513eb4b6-f027-47eb-c100-ac97dbb99d15"
   },
   "outputs": [],
   "source": [
    "# Test augmentation\n",
    "plt.figure(figsize=(7, 7))\n",
    "for images, labels in train_ds.take(1):\n",
    "  #images contains a batch of images\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(data_augmentation(images[i].numpy())) ## use .astype(\"uint8\") if images goes from 0 to 255 else float from 0 to 1\n",
    "    plt.title(class_names[np.argmax(labels[i])])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/SignLanguageRecognition/AlphabetRecognition/m/AL-ALPH1/v/AL-ALPH1-59\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 4 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 4 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/SignLanguageRecognition/AlphabetRecognition/m/AL-ALPH1/v/AL-ALPH1-59/metadata\n"
     ]
    }
   ],
   "source": [
    "#config model params\n",
    "model_params = {\n",
    "    \"dropout1\": 0.1,\n",
    "    \"dropout2\": 0.25,\n",
    "    \"dense\": 64,\n",
    "    \"l2reg\": 0.015\n",
    "}\n",
    "\n",
    "if using_neptune: # log model params to neptune\n",
    "    log_model_version = neptune.init_model_version(\n",
    "        model=\"AL-ALPH1\",\n",
    "        project=\"SignLanguageRecognition/AlphabetRecognition\",\n",
    "        api_token=neptune_api_token\n",
    "    )\n",
    "    log_model_version[\"params/model\"] = model_params\n",
    "\n",
    "    log_model_version_url = log_model_version.get_url()\n",
    "\n",
    "    log_model_version.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6399,
     "status": "ok",
     "timestamp": 1700666176602,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "IeT5Ya8qy15l",
    "outputId": "d781b176-1574-405b-b7bd-82a5ff832615"
   },
   "outputs": [],
   "source": [
    "# reset/init model with model params\n",
    "\n",
    "# VGG19\n",
    "#VGG19_model = tf.keras.applications.VGG19\n",
    "# Freeze the 6 first layers of the model to not train them\n",
    "#for layer in VGG19_model.layers[:6]:\n",
    "#  layer.trainable = False\n",
    "\n",
    "# base_model\n",
    "base_model = tf.keras.applications.InceptionV3(input_shape=(image_side,image_side,3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "  data_augmentation,\n",
    "  #VGG19_model,\n",
    "  base_model,\n",
    "\n",
    "##base\n",
    "  Flatten(),\n",
    "  Dropout(model_params[\"dropout1\"]),\n",
    "  Dense(model_params[\"dense\"], kernel_regularizer=regularizers.l2(model_params[\"l2reg\"]), activation = 'relu'),\n",
    "  Dropout(model_params[\"dropout2\"]),\n",
    "  Dense(len(class_names), activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.build((None, image_side, image_side, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "executionInfo": {
     "elapsed": 1079885,
     "status": "error",
     "timestamp": 1700667260329,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "x6ikkpdVRoJl",
    "outputId": "e0f29f40-3682-4e35-b1f2-f4fee2237a51"
   },
   "outputs": [],
   "source": [
    "## compile and train the model\n",
    "\n",
    "learning_rate = 0.0004\n",
    "epochs = 1\n",
    "\n",
    "# Log the run to Neptune\n",
    "if using_neptune:\n",
    "    run = neptune.init_run(project=\"SignLanguageRecognition/AlphabetRecognition\", api_token=neptune_api_token, capture_hardware_metrics=True, capture_stdout=True, capture_stderr=False)\n",
    "    params = {\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"base_model\": base_model.name,\n",
    "        \"image_side\": image_side,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": bs\n",
    "        }\n",
    "\n",
    "    run[\"parameters\"] = params\n",
    "    run[\"model_version_url\"] = log_model_version_url\n",
    "\n",
    "    ##too slow find a better way\n",
    "    #run[\"dataset/train\"].track_files(DATASET_PATH + \"/Train\")\n",
    "    #run[\"dataset/test\"].track_files(DATASET_PATH + \"/Test\")\n",
    "    run[\"dataset/current\"].track_files(DATASET_PATH + \"-current.tar\")\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer= SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model', verbose=1, save_best_only=True, monitor = 'val_accuracy', mode = 'max') # this save the best model\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.000001) # this reduce learning rate when val_loss is not improving\n",
    "#early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) # this stop training when val_loss is not improving\n",
    "\n",
    "def epochCallback(epoch, logs):\n",
    "    if using_neptune:\n",
    "        run[\"train/loss\"].log(logs[\"loss\"])\n",
    "        run[\"train/accuracy\"].log(logs[\"accuracy\"])\n",
    "        run[\"eval/loss\"].log(logs[\"val_loss\"])\n",
    "        run[\"eval/accuracy\"].log(logs[\"val_accuracy\"])\n",
    "\n",
    "history = None\n",
    "try:\n",
    "    history = model.fit(train_ds, validation_data = validation_ds, epochs=epochs,\n",
    "                                callbacks = [\n",
    "    #                                reduce_lr,\n",
    "                                    checkpointer,\n",
    "                                    tf.keras.callbacks.LambdaCallback(on_epoch_end=epochCallback)\n",
    "    #                                early_stop\n",
    "                                ])\n",
    "    if using_neptune:\n",
    "        model.save(\"current_run_model.h5\")\n",
    "        run[\"model_weights\"].upload(\"current_run_model.h5\")\n",
    "        run.stop()\n",
    "except:\n",
    "    if using_neptune:\n",
    "        run.stop() # make sure to stop the run if something goes wrong\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inceptionV3 test\n",
    "# this is a copy example from https://keras.io/api/applications/\n",
    "base_model = tf.keras.applications.InceptionV3\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer — let's say we have 200 classes\n",
    "predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "checkpointer = ModelCheckpoint(filepath='model', verbose=1, save_best_only=True, monitor = 'val_accuracy', mode = 'max')\n",
    "\n",
    "history = model.fit(train_ds, validation_data = validation_ds, epochs=1,\n",
    "                            callbacks = [\n",
    "                                checkpointer\n",
    "                            ])\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit(train_ds, validation_data = validation_ds, epochs=15,\n",
    "                            callbacks = [\n",
    "                                checkpointer\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow hyperparameter tuning\n",
    "\n",
    "##TODO batch size and image side tuning\n",
    "##TODO freeze layers tuning\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "resNet = tf.keras.applications.ResNet152(input_shape=(image_side,image_side,3),\n",
    "                                          include_top=False,\n",
    "                                          weights='imagenet')\n",
    "checkpointer = ModelCheckpoint(filepath='model', verbose=1, save_best_only=True, monitor = 'val_accuracy', mode = 'max')\n",
    "\n",
    "def model_builder(hp):\n",
    "\n",
    "    dropOut1 = hp.Float('dropout1', min_value=0.1, max_value=0.5, step=0.05)\n",
    "    dropOut2 = hp.Float('dropout2', min_value=0.1, max_value=0.5, step=0.05)\n",
    "    dense1 = hp.Int('dense1', min_value=32, max_value=256, step=32)\n",
    "    l2reg = hp.Float('l2reg', min_value=0.005, max_value=0.05, step=0.005)\n",
    "    lrate1 = hp.Float('learning_rate', min_value=0.0001, max_value=0.01, step=0.0001)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(data_augmentation)\n",
    "    model.add(resNet)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropOut1))\n",
    "    model.add(Dense(dense1, kernel_regularizer=regularizers.l2(l2reg), activation = 'relu'))\n",
    "    model.add(Dropout(dropOut2))\n",
    "    model.add(Dense(len(class_names), activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer= SGD(learning_rate=lrate1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                    objective='val_accuracy',\n",
    "                    max_epochs=epochs,\n",
    "                    factor=3,\n",
    "                    directory='my_dir',\n",
    "                    project_name='intro_to_kt')\n",
    "\n",
    "tuner.search(train_ds, validation_data = validation_ds, epochs=epochs, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1700665532547,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "Qvu8W0S3SQrC",
    "outputId": "f73080db-c4c3-4b95-90fd-a5b75da547c6"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val_loss','loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700665532548,
     "user": {
      "displayName": "Manuel Herrera Cruz",
      "userId": "11188891401280240326"
     },
     "user_tz": -60
    },
    "id": "wUo_hM58hRp-",
    "outputId": "21c1bfcf-a64f-4f14-c57b-1f381e18501f"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['val_accuracy','accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"letters_new5_model_192x192_74.0pc_inceptionv3\"\n",
    "model.save(name + \".keras\")\n",
    "model.save(name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model from zero\n",
    "model = tf.keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## generamos predicciones\n",
    "\n",
    "y_pred = []  # store predicted labels\n",
    "y_true = []  # store true labels\n",
    "\n",
    "# iterate over the dataset\n",
    "for image_batch, label_batch in validation_ds:   # use dataset.unbatch() with repeat\n",
    "   # append true labels\n",
    "   y_true.append(label_batch)\n",
    "   # compute predictions\n",
    "   preds = model.predict(image_batch)\n",
    "   # append predicted labels\n",
    "   y_pred.append(np.argmax(preds, axis = - 1))\n",
    "\n",
    "# convert the true and predicted labels into tensors\n",
    "correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
    "predicted_labels = tf.concat([item for item in y_pred], axis = 0)\n",
    "\n",
    "# convertimos el array en el que necesita sklearn\n",
    "final_correct_labels = []\n",
    "for lb in correct_labels:\n",
    "  itlist=list(lb.numpy())\n",
    "  final_correct_labels.append(itlist.index(1.0))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(final_correct_labels, predicted_labels, display_labels=class_names, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate failed predictions to see what is happening\n",
    "failed_predictions = []\n",
    "\n",
    "# iterate over the dataset\n",
    "for image_batch, label_batch in train_ds:   # train_ds or validation_ds\n",
    "   \n",
    "    # compute predictions\n",
    "    preds = model.predict(image_batch)\n",
    "\n",
    "    # append failed predictions to failed_predictions\n",
    "    for i in range(len(preds)):\n",
    "        if np.argmax(preds[i]) != np.argmax(label_batch[i]):\n",
    "#            if class_names[np.argmax(label_batch[i])] == 'N':\n",
    "                failed_predictions.append([image_batch[i], label_batch[i], preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rindex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show random image from failed predictions\n",
    "\n",
    "rindex += 1\n",
    "\n",
    "#check if rindex is defined\n",
    "if rindex >= len(failed_predictions):\n",
    "    rindex = 0\n",
    "\n",
    "print(\"Index of failed prediction: \", rindex)\n",
    "# show image with plt\n",
    "plt.title(\"True: \"+class_names[np.argmax(failed_predictions[rindex][1])]+\", Pred: \"+class_names[np.argmax(failed_predictions[rindex][2])])\n",
    "plt.imshow(failed_predictions[rindex][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_tester\n",
    "\n",
    "model_list = [\n",
    "\n",
    "    tf.keras.applications.ConvNeXtBase,\n",
    "    tf.keras.applications.ConvNeXtLarge,\n",
    "    tf.keras.applications.ConvNeXtSmall,\n",
    "    tf.keras.applications.ConvNeXtTiny,\n",
    "    tf.keras.applications.ConvNeXtXLarge,\n",
    "    tf.keras.applications.DenseNet121,\n",
    "    tf.keras.applications.DenseNet169,\n",
    "    tf.keras.applications.DenseNet201,\n",
    "    # tf.keras.applications.EfficientNetB0,\n",
    "    # tf.keras.applications.EfficientNetB1,\n",
    "    # tf.keras.applications.EfficientNetB2,\n",
    "    # tf.keras.applications.EfficientNetB3,\n",
    "    # tf.keras.applications.EfficientNetB4,\n",
    "    # tf.keras.applications.EfficientNetB5,\n",
    "    # tf.keras.applications.EfficientNetB6,\n",
    "    # tf.keras.applications.EfficientNetB7,\n",
    "    # tf.keras.applications.EfficientNetV2B0,\n",
    "    # tf.keras.applications.EfficientNetV2B1,\n",
    "    # tf.keras.applications.EfficientNetV2B2,\n",
    "    # tf.keras.applications.EfficientNetV2B3,\n",
    "    # tf.keras.applications.EfficientNetV2L,\n",
    "    # tf.keras.applications.EfficientNetV2M,\n",
    "    # tf.keras.applications.EfficientNetV2S,\n",
    "    tf.keras.applications.InceptionResNetV2,\n",
    "    tf.keras.applications.InceptionV3,\n",
    "    tf.keras.applications.MobileNet,\n",
    "    tf.keras.applications.MobileNetV2,\n",
    "    tf.keras.applications.MobileNetV3Large,\n",
    "    tf.keras.applications.MobileNetV3Small,\n",
    "    tf.keras.applications.NASNetLarge,\n",
    "    tf.keras.applications.NASNetMobile,\n",
    "    tf.keras.applications.RegNetX002,\n",
    "    tf.keras.applications.RegNetX004,\n",
    "    tf.keras.applications.RegNetX006,\n",
    "    tf.keras.applications.RegNetX008,\n",
    "    tf.keras.applications.RegNetX016,\n",
    "    tf.keras.applications.RegNetX032,\n",
    "    tf.keras.applications.RegNetX040,\n",
    "    tf.keras.applications.RegNetX064,\n",
    "    tf.keras.applications.RegNetX080,\n",
    "    tf.keras.applications.RegNetX120,\n",
    "    tf.keras.applications.RegNetX160,\n",
    "    tf.keras.applications.RegNetX320,\n",
    "    tf.keras.applications.RegNetY002,\n",
    "    tf.keras.applications.RegNetY004,\n",
    "    tf.keras.applications.RegNetY006,\n",
    "    tf.keras.applications.RegNetY008,\n",
    "    tf.keras.applications.RegNetY016,\n",
    "    tf.keras.applications.RegNetY032,\n",
    "    tf.keras.applications.RegNetY040,\n",
    "    tf.keras.applications.RegNetY064,\n",
    "    tf.keras.applications.RegNetY080,\n",
    "    tf.keras.applications.RegNetY120,\n",
    "    tf.keras.applications.RegNetY160,\n",
    "    tf.keras.applications.RegNetY320,\n",
    "    tf.keras.applications.ResNet101,\n",
    "    tf.keras.applications.ResNet152,\n",
    "    tf.keras.applications.ResNet50,\n",
    "    tf.keras.applications.ResNetRS101,\n",
    "    tf.keras.applications.ResNetRS152,\n",
    "    tf.keras.applications.ResNetRS200,\n",
    "    tf.keras.applications.ResNetRS270,\n",
    "    tf.keras.applications.ResNetRS350,\n",
    "    tf.keras.applications.ResNetRS420,\n",
    "    tf.keras.applications.ResNetRS50,\n",
    "    tf.keras.applications.ResNet101V2,\n",
    "    tf.keras.applications.ResNet152V2,\n",
    "    tf.keras.applications.ResNet50V2,\n",
    "    tf.keras.applications.VGG16,\n",
    "    tf.keras.applications.VGG19,\n",
    "    tf.keras.applications.Xception\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(base_model, model_params):\n",
    "    # Create the model\n",
    "    model = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        #VGG19_model,\n",
    "        base_model,\n",
    "\n",
    "        ##base\n",
    "        Flatten(),\n",
    "        Dropout(model_params[\"dropout1\"]),\n",
    "        Dense(model_params[\"dense\"], kernel_regularizer=regularizers.l2(model_params[\"l2reg\"]), activation = 'relu'),\n",
    "        Dropout(model_params[\"dropout2\"]),\n",
    "        Dense(len(class_names), activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    model.build((None, image_side, image_side, 3))\n",
    "    return model\n",
    "\n",
    "def train(base_model, model, epochs, learning_rate, log_model_version_url):\n",
    "    # Log the run to Neptune\n",
    "    run = neptune.init_run(project=\"SignLanguageRecognition/AlphabetRecognition\", api_token=neptune_api_token, capture_hardware_metrics=True, capture_stdout=True, capture_stderr=True)\n",
    "    params = {\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"base_model\": base_model.name,\n",
    "        \"image_side\": image_side,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": bs\n",
    "        }\n",
    "\n",
    "    run[\"parameters\"] = params\n",
    "    run[\"model_version_url\"] = log_model_version_url\n",
    "    run[\"status\"] = \"running\"\n",
    "\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer= SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    def epochCallback(epoch, logs):\n",
    "        run[\"train/loss\"].log(logs[\"loss\"])\n",
    "        run[\"train/accuracy\"].log(logs[\"accuracy\"])\n",
    "        run[\"eval/loss\"].log(logs[\"val_loss\"])\n",
    "        run[\"eval/accuracy\"].log(logs[\"val_accuracy\"])\n",
    "\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) # this stop training when val_loss is not improving\n",
    "\n",
    "    try:\n",
    "        model.fit(train_ds, validation_data = validation_ds, epochs=epochs,\n",
    "                                    callbacks = [\n",
    "                                        tf.keras.callbacks.LambdaCallback(on_epoch_end=epochCallback),\n",
    "                                        early_stop\n",
    "                                    ])\n",
    "        if using_neptune:\n",
    "            run[\"status\"] = \"finished\"\n",
    "            model.save(\"current_run_model.h5\")\n",
    "            run[\"model_weights\"].upload(\"current_run_model.h5\")\n",
    "            \n",
    "            run.stop()\n",
    "\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        if using_neptune:\n",
    "            run[\"status\"] = \"crashed-ResourceExhausted\"\n",
    "            run.stop() # make sure to stop the run if something goes wrong\n",
    "    except KeyboardInterrupt:\n",
    "        if using_neptune:\n",
    "            run[\"status\"] = \"Interrupted keyboard\"\n",
    "            run.stop() # make sure to stop the run if something goes wrong\n",
    "        raise\n",
    "\n",
    "for base_model_func in model_list:\n",
    "\n",
    "    base_model = base_model_func(input_shape=(image_side,image_side,3), include_top=False, weights='imagenet')\n",
    "\n",
    "    #config model params\n",
    "    model_params = {\n",
    "        \"dropout1\": 0.1,\n",
    "        \"dropout2\": 0.25,\n",
    "        \"dense\": 64,\n",
    "        \"l2reg\": 0.015\n",
    "    }\n",
    "\n",
    "    log_model_version = neptune.init_model_version(\n",
    "        model=\"AL-ALPH1\",\n",
    "        project=\"SignLanguageRecognition/AlphabetRecognition\",\n",
    "        api_token=neptune_api_token\n",
    "    )\n",
    "    log_model_version[\"params/model\"] = model_params\n",
    "\n",
    "    log_model_version_url = log_model_version.get_url()\n",
    "\n",
    "    log_model_version.stop()\n",
    "\n",
    "\n",
    "    model = build(base_model, model_params)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50\n",
    "\n",
    "    train(base_model, model, epochs, learning_rate, log_model_version_url)\n",
    "\n",
    "    del model\n",
    "    del base_model\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
